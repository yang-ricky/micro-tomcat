# 任务14：集群支持实现清单（优化版）

## 整体目标
在 Micro Tomcat 的基础上，支持多节点集群部署，实现会话共享、负载均衡、故障检测和自动恢复，并提供集群状态监控接口。

---

## 14.1 节点注册与集群配置
### 基础设施
- [x] **`ClusterNode` 类**：包含节点名称、IP、端口、状态等基本信息  
- [x] **`ClusterConfigLoader` 类**：实现节点配置加载（从 XML 或启动参数）  
- [x] **`ClusterRegistry` 类**：维护集群节点列表，对外提供节点查询与注册接口  

### 配置文件
- [x] 创建 `cluster-config.xml` 模板  
- [x] **`ClusterConfigParser` 类**：解析 `cluster-config.xml`，返回节点信息  
- [x] 在 `ClusterRegistry` 中添加节点配置验证逻辑  

### 验证要求
- [x] 启动时正确打印所有节点信息  
- [x] 成功加载并验证集群配置  
- [x] 节点能够正确注册到集群中  

---

## 14.2 心跳机制与故障检测
### 心跳实现
- [x] **`HeartbeatService` 接口**：定义心跳检测方法  
- [x] **`DefaultHeartbeatService` 类**：实现心跳线程/定时器  
- [x] 在每个节点提供 `/ping` 健康检查接口  
- [x] 添加心跳超时配置（可在 `cluster-config.xml` 中定义）  

### 状态管理
- [x] **`NodeStatusManager` 类**：根据心跳结果更新节点状态  
- [x] 添加节点状态变更事件（如 `onNodeDown()`, `onNodeUp()`）  
- [x] 在 `ClusterRegistry` 中维护节点可用性标记  

### 验证要求
- [ ] 正确记录心跳检测日志  
- [ ] 准确标记故障节点状态  
- [ ] 能够检测到节点恢复  

---

## 14.3 分布式会话存储
### 存储实现
- [ ] **`DistributedSessionManager` 类**：负责分布式会话管理  
- [ ] **`SessionStoreAdapter` 接口**：定义访问底层存储的方法  
- [ ] **`RedisSessionStore` 类**（示例）：实现基于 Redis 的会话存储  
- [ ] 会话同步机制（可在更新会话后即时写入底层存储）

### 会话管理
- [ ] 重写 `getSession()` 和 `createSession()` 方法，统一从 `SessionStoreAdapter` 获取或创建会话  
- [ ] 实现会话数据序列化/反序列化（可使用 JSON 或 Java 序列化）  
- [ ] 添加会话过期处理（可由 Redis TTL 或定时扫描过期）  

### 验证要求
- [ ] 会话能在不同节点间共享  
- [ ] 节点故障时会话数据不丢失  
- [ ] 会话同步性能满足要求  

---

## 14.4 负载均衡策略
### 基础实现
- [ ] **`LoadBalancer` 接口**：定义选择节点的方法：`ClusterNode selectNode(Request req)`  
- [ ] **`RoundRobinLoadBalancer` 类**：实现轮询策略  
- [ ] **`LeastConnectionsLoadBalancer` 类**：实现最少连接策略  
- [ ] **`HashLoadBalancer` 类**：实现哈希策略（根据 IP 或 Session ID）  

### 高级特性
- [ ] 会话粘性支持：在 `LoadBalancer` 内根据 Session ID 决定节点（可与 Hash 策略结合）  
- [ ] 故障节点自动跳过：当节点标记为故障时，负载均衡不再分配请求给该节点  
- [ ] 请求重试机制：若目标节点无法访问，自动重试下一个可用节点  

### 验证要求
- [ ] 请求能均匀分布到各节点（轮询或最少连接）  
- [ ] 故障节点自动排除，避免无效请求  
- [ ] 会话粘性正常工作，确保同一用户会话落在同一节点  

---

## 14.5 故障检测与自动恢复
### 故障处理
- [ ] **`FailureDetector` 接口**：定义故障判定和事件触发  
- [ ] **`DefaultFailureDetector` 类**：集成心跳结果，实现故障事件通知  
- [ ] 故障节点资源清理（如释放连接池、内存占用等）  
- [ ] 实现请求重路由：故障节点不可用后，及时将请求转发到其他节点  

### 自动恢复
- [ ] 节点重新注册：故障节点恢复后，向 `ClusterRegistry` 通知自身上线  
- [ ] 资源重新分配：在分布式会话或缓存层重建所需数据  
- [ ] 实现配置动态更新：当集群拓扑变更时，动态刷新 `LoadBalancer`、`SessionStoreAdapter` 等组件  

### 验证要求
- [ ] 故障转移过程无明显服务中断  
- [ ] 节点恢复后自动加入集群  
- [ ] 完整的故障处理日志记录  

---

## 14.6 集群状态监控接口
### API实现
- [ ] **`ClusterStatusController` 类**：提供 `/cluster/status` REST接口  
- [ ] **`ClusterMBean` 接口**：定义集群相关 MBean 接口（查看节点状态、请求统计等）  
- [ ] **`ClusterStatusMBean` 类**：实现上述接口，并注册到 MBeanServer  

### 监控指标
- [ ] 节点存活状态、CPU/内存使用、当前连接数或请求数  
- [ ] 会话数量、会话创建/销毁速率  
- [ ] 网络流量、吞吐量等性能指标（可选）  

### 验证要求
- [ ] `/cluster/status` 返回完整的集群状态（JSON 或 XML）  
- [ ] JMX 监控正常工作，可通过 JConsole 等工具查看状态  
- [ ] 监控数据实时更新，不遗漏故障与恢复事件  

---

## 14.7 高级会话同步与 Gossip 协议
1. **引入 Gossip 协议**  
   - [ ] **`GossipManager` 类**：负责与其他节点周期性通信，随机选择一部分节点交换"session 元信息"或"节点健康状态"  
   - [ ] **`GossipProtocol` 接口**：抽象出 `sendGossip(...)`、`receiveGossip(...)` 方法，便于不同实现或扩展  
   - [ ] **周期任务**：每隔 X 秒(如 2~5 秒)，随机选择若干节点，用简单的"push-pull"方式交换 session ID 列表、最后一次更新时间等  
   - [ ] **数据合并策略**：如果本地没有某会话或更新版本较老，就向对方请求完整数据（JSON 或二进制），然后更新本地 Session

2. **Session 元数据**  
   - [ ] 在 `Session` 对象里添加**版本号**或**时间戳**字段，比如 `version` (long) 或 `VectorClock`  
   - [ ] 当节点对 Session 做任何写操作(`setAttribute`, `removeAttribute`)，`version++` 或更新时间戳，以便在 Gossip 时比较谁更新得更晚

3. **最终一致性**  
   - [ ] 设计简易的**"last write wins"**策略：  
     - 比较 `Session.version`(或时间戳)，保留较大的那份 Session  
     - 这样可以**在足够多的 Gossip 轮次后**，让所有节点最终收敛到同一个状态  
   - [ ] （可选）引入 `VectorClock`：若检测到并发更新冲突（版本向量存在分支），则需要更复杂的冲突解决策略

4. **故障节点与网络分区**  
   - [ ] **在 Gossip 消息中**也携带节点状态信息：是否在线、最后一次响应时间等  
   - [ ] **当节点恢复**（重新可达）时，通过 Gossip **自动获取**最新会话数据（或者至少会话元数据）  
   - [ ] 如果网络出现分区，处于不同分区的节点可能各自有独立更新；**当分区恢复**后，需要通过 Gossip 进行合并

**验证要求：**  
- 当多个节点**频繁更新**同一个 Session 时，最后会在若干轮 Gossip 后收敛到一致状态（采用简单"last write wins" 或带时间戳的策略）  
- 故障节点恢复后，能自动**向其他节点**拉取最新 Session 数据，不导致会话丢失

## 14.8 避免并发更新冲突的版本向量 / 分布式日志
1. **版本向量** (Vector Clock)
   - [ ] 在 `Session` 中添加一个 `Map<String, Long>` 类型的 `vectorClock`，其中 key 是 **节点 ID**，value 是该节点对 Session 的更新序号  
   - [ ] 每次写入 session，**只增加**本节点对应的 `vectorClock[nodeId]`，以标明"来自节点 X 的第 N 次更新"  
   - [ ] 在 Session 合并时，**比较**两个 vectorClock，看是否一方**完全因果先于**另一方(`<=`)；若不是，则产生**并发冲突**  
   - [ ] 当发生并发冲突，可选择**合并策略**或**保留多版本**（让应用层处理冲突）  

2. **分布式日志**  
   - [ ] 除了对 Session 做"最终态"复制，你也可以记录**每一次属性写操作**到一个日志里（类似 Operation Log）  
   - [ ] Gossip 时仅交换日志条目，根据**Operation ID**或**操作序号**来判断是否应用过该更新  
   - [ ] 这种**日志复制**方式更容易处理**部分更新冲突**（按操作顺序应用）  

3. **冲突检测与分辨**  
   - [ ] 当 `vectorClock` 检测到两个并发更新都不是因果先于对方（发生分叉），可**尝试**自动合并或**手动**保留分支  
   - [ ] 若是**最后写覆盖**（LWW），就对比节点版本最大的保留；若想支持**合并**（如购物车添加商品），可在属性级别做"并集"操作

**验证要求：**  
- 发生**并发写**时（两个节点几乎同时写同一个 Session），能够检测到冲突，或者做相应的处理（合并/最后写覆盖/多版本）  
- 会话属性在网络分区并发更新下不会**莫名丢失**（至少保留多版本，或提示冲突）  

## 14.9 Leader 选举（Raft/Paxos）——强一致性存储
1. **Leader-based 架构**  
   - [ ] 在分布式 Session 中，选举**一个 Leader**节点  
   - [ ] 所有对 Session 的写请求先发到 Leader，再由 Leader 复制到 Follower（多数派确认后才算成功）  
   - [ ] Leader 故障时，由 Paxos/Raft 协议发起新一轮选举  
   - [ ] 这样可以保证**严格一致性**（CP），但牺牲部分可用性（故障期间不能写）  

2. **Raft 协议**  
   - [ ] 维护一个 **`Term`**、**`LeaderId`**、**`Log`** 数组  
   - [ ] 当节点启动或 Leader 心跳超时，发起选举  
   - [ ] Leader 将每次写操作以"Log entry"形式复制给多数节点后，再提交  
   - [ ] Session 更新变为**写入 Raft 日志**，一旦提交就表示多个节点都持久化  
   - [ ] 需要**RaftNode** 类，内部包含`state`、`role`(Leader、Candidate、Follower) 等字段

3. **一致性 / 可用性权衡**  
   - [ ] Raft/Paxos 能提供 CP（一致性、分区容忍）但可用性会受影响：若少于半数节点在线，就无法写  
   - [ ] 同时，这个模式在**网络分区**时会阻止被少数派分区改写 Session，避免出现冲突

**验证要求：**  
- Leader 故障后，能自动选出新的 Leader，原先的会话数据不会丢失  
- 确保在多数派存活时，写请求能继续进行；少数派分区被阻止写操作，避免分叉  
- 符合强一致：所有已提交的操作对外呈现相同结果  

## 14.10 节点重入、网络分区恢复
1. **节点重入**  
   - [ ] 当节点下线或出现故障后，过一段时间又恢复上线，需要**同步**已变更的 Session 数据  
   - [ ] 在 Gossip 模式下，通过周期交换向量版本或日志条目，自动让恢复节点追上最新状态  
   - [ ] 在 Raft 模式下，恢复节点会从 Leader 拉取缺失的日志条目

2. **网络分区**  
   - [ ] 当网络分成多个子网时，不同分区内的节点可能继续更新数据  
   - [ ] **最终一致**场景下，分区恢复后要进行**大量合并**或**冲突检测**  
   - [ ] **强一致**场景下，如果在少数派分区的 Leader 被隔离，就无法写操作；恢复时需要更新正确的日志

3. **节点重入的合并策略**  
   - [ ] Gossip: 恢复节点先通过**全量 sync** 或 **pull missing** logs / sessions  
   - [ ] Raft: 恢复节点以 `NextIndex` / `MatchIndex` 去从 Leader 获取已提交的日志  
   - [ ] 处理**同时**多个节点恢复时的极端情况

**验证要求：**  
- 当一个节点长时间下线后重新上线，不会导致数据回退或发生严重冲突；能与集群重新对账同步  
- 出现网络分区时，如果是最终一致，允许在分区内各自更新；分区恢复后有正确的合并流程  
- 如果是强一致，那么少数派分区写操作会被拒绝或阻塞

## 14.11 一致性级别配置
1. **系统配置**  
   - [ ] 增加 `distributed.consistencyLevel` 参数，可选值：  
     - `STRONG`：使用 Raft/Paxos  
     - `EVENTUAL`：使用 Gossip + last-write-wins / Vector Clock  
     - `READ_LOCAL_WRITE_BROADCAST`：简单的本地立即写 + 广播，不保证同时并发写的一致性  
   - [ ] 根据用户配置，在初始化时加载相应的**SessionStoreAdapter** / **Replication Protocol** / **故障检测**

2. **可配置一致性**  
   - [ ] 提供**API 或管理界面**，让运维人员在**不重启**系统的情况下切换一致性策略  
   - [ ] 当切换到 `STRONG`，可能需要等待节点达到共识  
   - [ ] 当切换到 `EVENTUAL`，可以停掉 Raft 线程并启动 Gossip

3. **混合部署场景**  
   - [ ] 有些应用对会话一致性要求不高，采用 `EVENTUAL`；另一些关键组件（如交易）可引入 `STRONG`  
   - [ ] 分别在应用层 / context 层配置，或者在 SessionManager 级别配置

**验证要求：**  
- 可以在配置文件或 CLI 参数中**指定**一致性等级  
- 切换一致性模式后，集群中各节点能正常加载对应协议并运转  
- 不同场景（高可用/最终一致 vs 强一致）都能正确演示

## 14.12 小结
通过这些**进阶任务**，新手可以**逐步学习**：  
1. **Gossip 协议**的 push-pull 模式、版本向量 (vector clock) 检测并发写、合并策略；  
2. **Raft/Paxos** 等强一致协议的核心概念：**日志复制**、**选举**、**多数派写入**；  
3. **网络分区**下的可用性 vs 一致性选择（CAP 原理），以及**节点恢复**、**数据同步**过程；  
4. 根据业务需要，确定合适的一致性级别（`EVENTUAL`, `STRONG`, `READ_LOCAL_WRITE_BROADCAST` 等）。

完成后，你就可以在**Micro Tomcat**里实现**真正的分布式** Session 存储和**更加完善**的故障处理机制，具备了分布式系统的核心概念。这对深入理解**分布式理论**和**实际工程应用**都有很大帮助。祝学习与实践愉快!